{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/60 [02:50<2:47:57, 170.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss = 0.5984\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTModel\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0';'8.6'\n",
    "\n",
    "haar_cascade_path = \"D:/DeepFake/nawfal/haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
    "\n",
    "class ViT_StyleGAN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViT_StyleGAN_Model, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, c, h, w = x.size()\n",
    "        x = x.view(batch_size, c, h, w)\n",
    "        features = self.vit(pixel_values=x).last_hidden_state[:, 0, :]  # Extract the [CLS] token\n",
    "        features = features.view(batch_size, -1)\n",
    "        out = self.fc(features)  # No need for indexing here, as `features` is already 2D\n",
    "        return out\n",
    "\n",
    "\n",
    "def load_stylegan_models():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    network_pkl = \"D:\\\\DeepFake\\\\stylegan2-ffhq-1024x1024.pkl\"\n",
    "    with dnnlib.util.open_url(network_pkl) as f:\n",
    "        network = legacy.load_network_pkl(f)\n",
    "        G = network['G_ema'].to(device)  # Generator\n",
    "        D = network['D'].to(device)  # Discriminator\n",
    "    return G, D\n",
    "\n",
    "def generate_images_with_gan(G, num_images):\n",
    "    device = next(G.parameters()).device  # Ensure to get the device from the generator's parameters\n",
    "    z = torch.randn(num_images, G.z_dim).to(device)\n",
    "    c = torch.zeros(num_images, G.c_dim).to(device)\n",
    "    images = G(z, c, truncation_psi=0.7, noise_mode='const')\n",
    "\n",
    "    # Resize images to 224x224\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "\n",
    "    images_resized = [resize_transform(image.cpu().detach()) for image in images]\n",
    "    images_resized = torch.stack([transforms.ToTensor()(image).to(device) for image in images_resized])  # Move to GPU\n",
    "    return images_resized\n",
    "\n",
    "def evaluate_images_with_discriminator(D, images):\n",
    "    device = next(D.parameters()).device  # Get the device from the discriminator's parameters\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = D(images)\n",
    "    return logits\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_folder, label, transform=None, max_frames=100):\n",
    "        self.video_folder = video_folder\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        self.video_files = [os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            video_path = self.video_files[idx]\n",
    "            frames = self.load_video(video_path)\n",
    "            if self.transform:\n",
    "                frames = [self.transform(frame) for frame in frames]\n",
    "            frames = torch.stack(frames)\n",
    "            label = torch.tensor(self.label, dtype=torch.long)\n",
    "            return frames, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video {self.video_files[idx]}: {e}\")\n",
    "            return torch.zeros((self.max_frames, 3, 224, 224)), torch.tensor(self.label, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = faces[0]\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                face = cv2.resize(face, (224, 224))\n",
    "                frames.append(face)\n",
    "\n",
    "        cap.release()\n",
    "        frames = self.pad_or_truncate_frames(frames)\n",
    "        return frames\n",
    "\n",
    "    def pad_or_truncate_frames(self, frames):\n",
    "        if len(frames) < self.max_frames:\n",
    "            while len(frames) < self.max_frames:\n",
    "                frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "        else:\n",
    "            frames = frames[:self.max_frames]\n",
    "        return frames\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "VIDEO_DIR_REAL =r\"D:/New folder/real\"\n",
    "VIDEO_DIR_MANIPULATED =r\"D:/New folder/fake\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "real_dataset = VideoDataset(video_folder=VIDEO_DIR_REAL, label=0, transform=transform)\n",
    "manipulated_dataset = VideoDataset(video_folder=VIDEO_DIR_MANIPULATED, label=1, transform=transform)\n",
    "full_dataset = ConcatDataset([real_dataset, manipulated_dataset])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, labels = zip(*batch)\n",
    "    frames = torch.stack(frames)\n",
    "    labels = torch.tensor(labels)\n",
    "    return frames, labels\n",
    "\n",
    "data_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "model = ViT_StyleGAN_Model(num_classes=NUM_CLASSES)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "G, D = load_stylegan_models()\n",
    "\n",
    "def train_model(model, data_loader, num_epochs, save_dir=\"checkpoints\"):\n",
    "    model.train()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory to save checkpoints\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} started...\")\n",
    "\n",
    "        for batch_idx, (frames, labels) in enumerate(tqdm(data_loader)):\n",
    "            frames = frames.to(device)  # Move frames to GPU\n",
    "            labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate and resize images with StyleGAN\n",
    "            gan_frames = generate_images_with_gan(G, frames.size(0))\n",
    "            gan_frames = gan_frames.to(device)  # Move GAN generated frames to GPU\n",
    "\n",
    "            outputs = model(gan_frames)  # Model expects inputs on the same device\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Batch {batch_idx + 1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Call the modified train function\n",
    "\n",
    "train_model(model, data_loader, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "model_save_path = \"vit_stylegan_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "model = ViT_StyleGAN_Model(num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "model.to(device)  # Ensure the model is on GPU\n",
    "model.eval()\n",
    "print(\"Model loaded from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
