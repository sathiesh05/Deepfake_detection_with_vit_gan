{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:253: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "  2%|▏         | 1/60 [01:46<1:44:42, 106.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss = 0.7179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [13:06<57:47, 70.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Loss = 0.6280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [24:04<38:49, 59.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: Loss = 0.5513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [35:54<35:15, 72.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: Loss = 0.6120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [49:37<22:34, 71.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: Loss = 0.7642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [1:02:51<12:58, 86.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: Loss = 0.9527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:13:25<00:00, 73.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6285\n",
      "Checkpoint saved: checkpoints\\model_epoch_1.pth\n",
      "Epoch 2/5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [00:54<53:09, 54.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss = 0.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [12:42<55:13, 67.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Loss = 0.7559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [33:34<1:24:16, 129.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: Loss = 0.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [2:09:13<1:35:16, 197.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: Loss = 0.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [2:20:42<22:35, 71.33s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: Loss = 0.5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [2:31:53<11:02, 73.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: Loss = 0.7557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [2:42:39<00:00, 162.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.6190\n",
      "Checkpoint saved: checkpoints\\model_epoch_2.pth\n",
      "Epoch 3/5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [01:11<1:10:24, 71.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss = 0.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [13:38<1:01:59, 75.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Loss = 0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [25:26<47:55, 73.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: Loss = 0.7829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [38:02<37:47, 78.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: Loss = 0.5647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [50:51<23:27, 74.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: Loss = 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [1:04:24<11:46, 78.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: Loss = 0.5649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:14:12<00:00, 74.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.6163\n",
      "Checkpoint saved: checkpoints\\model_epoch_3.pth\n",
      "Epoch 4/5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [00:50<49:46, 50.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss = 0.5798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [13:25<1:05:02, 79.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Loss = 0.5832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [24:06<41:47, 64.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: Loss = 0.4671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [34:43<31:45, 65.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: Loss = 0.7554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [45:26<20:09, 63.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: Loss = 0.5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [56:31<10:26, 69.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: Loss = 0.5663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:07:09<00:00, 67.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.6176\n",
      "Checkpoint saved: checkpoints\\model_epoch_4.pth\n",
      "Epoch 5/5 started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [01:06<1:05:08, 66.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Loss = 0.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11/60 [12:41<53:59, 66.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Loss = 0.7643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [24:57<47:38, 73.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: Loss = 0.3986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [36:31<33:41, 69.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: Loss = 0.5657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [49:29<23:45, 75.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: Loss = 0.5642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [1:02:01<12:17, 82.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: Loss = 0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [1:15:09<00:00, 75.16s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.6175\n",
      "Checkpoint saved: checkpoints\\model_epoch_5.pth\n",
      "Training complete.\n",
      "Model saved to vit_stylegan_model.pth\n",
      "Model loaded from disk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTModel\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0';'8.6'\n",
    "\n",
    "haar_cascade_path = \"haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
    "\n",
    "class ViT_StyleGAN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViT_StyleGAN_Model, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, c, h, w = x.size()\n",
    "        x = x.view(batch_size, c, h, w)\n",
    "        features = self.vit(pixel_values=x).last_hidden_state[:, 0, :]  # Extract the [CLS] token\n",
    "        features = features.view(batch_size, -1)\n",
    "        out = self.fc(features)  # No need for indexing here, as `features` is already 2D\n",
    "        return out\n",
    "\n",
    "\n",
    "def load_stylegan_models():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    network_pkl = \"stylegan2-ffhq-1024x1024.pkl\"\n",
    "    with dnnlib.util.open_url(network_pkl) as f:\n",
    "        network = legacy.load_network_pkl(f)\n",
    "        G = network['G_ema'].to(device)  # Generator\n",
    "        D = network['D'].to(device)  # Discriminator\n",
    "    return G, D\n",
    "\n",
    "def generate_images_with_gan(G, num_images):\n",
    "    device = next(G.parameters()).device  # Ensure to get the device from the generator's parameters\n",
    "    z = torch.randn(num_images, G.z_dim).to(device)\n",
    "    c = torch.zeros(num_images, G.c_dim).to(device)\n",
    "    images = G(z, c, truncation_psi=0.7, noise_mode='const')\n",
    "\n",
    "    # Resize images to 224x224\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "\n",
    "    images_resized = [resize_transform(image.cpu().detach()) for image in images]\n",
    "    images_resized = torch.stack([transforms.ToTensor()(image).to(device) for image in images_resized])  # Move to GPU\n",
    "    return images_resized\n",
    "\n",
    "def evaluate_images_with_discriminator(D, images):\n",
    "    device = next(D.parameters()).device  # Get the device from the discriminator's parameters\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = D(images)\n",
    "    return logits\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_folder, label, transform=None, max_frames=100):\n",
    "        self.video_folder = video_folder\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        self.video_files = [os.path.join(video_folder, f) for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            video_path = self.video_files[idx]\n",
    "            frames = self.load_video(video_path)\n",
    "            if self.transform:\n",
    "                frames = [self.transform(frame) for frame in frames]\n",
    "            frames = torch.stack(frames)\n",
    "            label = torch.tensor(self.label, dtype=torch.long)\n",
    "            return frames, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video {self.video_files[idx]}: {e}\")\n",
    "            return torch.zeros((self.max_frames, 3, 224, 224)), torch.tensor(self.label, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = faces[0]\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                face = cv2.resize(face, (224, 224))\n",
    "                frames.append(face)\n",
    "\n",
    "        cap.release()\n",
    "        frames = self.pad_or_truncate_frames(frames)\n",
    "        return frames\n",
    "\n",
    "    def pad_or_truncate_frames(self, frames):\n",
    "        if len(frames) < self.max_frames:\n",
    "            while len(frames) < self.max_frames:\n",
    "                frames.append(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "        else:\n",
    "            frames = frames[:self.max_frames]\n",
    "        return frames\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "VIDEO_DIR_REAL =r\"New folder/real\"\n",
    "VIDEO_DIR_MANIPULATED =r\"New folder/fake\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "real_dataset = VideoDataset(video_folder=VIDEO_DIR_REAL, label=0, transform=transform)\n",
    "manipulated_dataset = VideoDataset(video_folder=VIDEO_DIR_MANIPULATED, label=1, transform=transform)\n",
    "full_dataset = ConcatDataset([real_dataset, manipulated_dataset])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, labels = zip(*batch)\n",
    "    frames = torch.stack(frames)\n",
    "    labels = torch.tensor(labels)\n",
    "    return frames, labels\n",
    "\n",
    "data_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "model = ViT_StyleGAN_Model(num_classes=NUM_CLASSES)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "G, D = load_stylegan_models()\n",
    "\n",
    "def train_model(model, data_loader, num_epochs, save_dir=\"checkpoints\"):\n",
    "    model.train()\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory to save checkpoints\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} started...\")\n",
    "\n",
    "        for batch_idx, (frames, labels) in enumerate(tqdm(data_loader)):\n",
    "            frames = frames.to(device)  # Move frames to GPU\n",
    "            labels = labels.to(device)  # Move labels to GPU\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate and resize images with StyleGAN\n",
    "            gan_frames = generate_images_with_gan(G, frames.size(0))\n",
    "            gan_frames = gan_frames.to(device)  # Move GAN generated frames to GPU\n",
    "\n",
    "            outputs = model(gan_frames)  # Model expects inputs on the same device\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Batch {batch_idx + 1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Call the modified train function\n",
    "\n",
    "train_model(model, data_loader, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "model_save_path = \"vit_stylegan_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "model = ViT_StyleGAN_Model(num_classes=NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
    "model.to(device)  # Ensure the model is on GPU\n",
    "model.eval()\n",
    "print(\"Model loaded from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
